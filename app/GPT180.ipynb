{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machines of fnite depth: towards a\n",
      "formalization of neural networks\n",
      "Pietro Vertechi 1\n",
      "Mattia G. Bergomi 2\n",
      "Abstract\n",
      "We provide a unifying framework where artifcial neural networks and\n",
      "their architectures can be formally described as particular cases of a gen-\n",
      "eral mathematical construction—machines of fnite depth. Unlike neural\n",
      "networks, machines have a precise defnition, from which several proper-\n",
      "ties follow naturally. Machines of fnite depth are modular (they can be\n",
      "combined), efciently computable and diferentiable. The backward pass\n",
      "of a machine is again a machine and can be computed without overhead\n",
      "using the same procedure as the forward pass. We prove this statement\n",
      "theoretically and practically, via a unifed implementation that generalizes\n",
      "several classical architectures—dense, convolutional, and recurrent neural\n",
      "networks with a rich shortcut structure—and their respective backpropa-\n",
      "gation rules.\n",
      "1\n",
      "Introduction\n",
      "The notion of artifcial neural network has become more and more ill-defned\n",
      "over time. Unlike the initial defnitions [23], which could be easily formalized as\n",
      "directed graphs, modern neural networks can have the most diverse structures\n",
      "and do not obey a precise mathematical defnition.\n",
      "Defning a deep neural network is a practical question, which must be ad-\n",
      "dressed by all deep learning software libraries. Broadly, two solutions have been\n",
      "proposed. The simplest approach defnes a deep neural network as a stack of\n",
      "pre-built layers. The user can select among a large variety of pre-existing layers\n",
      "1Correspondence at pietro.vertechi@protonmail.com\n",
      "2Correspondence at mattiagbergomi@gmail.com\n",
      "1\n",
      "arXiv:2204.12786v1  [cs.LG]  27 Apr 2022\n",
      "and defne in what order to compose them. This approach simplifes the end-\n",
      "user’s mental load: in principle, it becomes possible for the user to confgure the\n",
      "model via a simplifed domain-specifc language. It also leads to computation-\n",
      "ally efcient models, as the rigid structure of the program makes its optimiza-\n",
      "tion easier for the library’s software developers. Unfortunately, this approach\n",
      "can quickly become limiting and prevent users from exploring more innova-\n",
      "tive architectures [2]. At the opposite end of the spectrum, a radically diferent\n",
      "approach, diferentiable programming [30], posits that every code is a model,\n",
      "provided that it can be diferentiated by an automatic diferentiation engine [10,\n",
      "13, 19, 20, 21, 24, 27]. This is certainly a promising direction, which has led\n",
      "to a number of technological advances, ranging from diferentiable ray-tracers\n",
      "to neural-network-based solvers for partial diferential equations [13, 22, 34].\n",
      "Unfortunately, this approach has several drawbacks, some practical and some\n",
      "theoretical. On the practical side, it becomes difcult to optimize the runtime of\n",
      "the forward and backward pass of an automatically-diferentiated, complex, un-\n",
      "structured code. On the other hand, a mathematical formalization would allow\n",
      "for an efcient, unifed implementation. From a more theoretical perspective,\n",
      "the space of models becomes somewhat ill-defned, as it is now the space of all\n",
      "diferentiable codes—not a structured mathematical space. This concern is not ex-\n",
      "clusively theoretical. A well-behaved smooth space of neural networks would\n",
      "be invaluable for automated diferentiable architecture search [17], where the\n",
      "optimal network structure for a given problem is found automatically. Further-\n",
      "more, a well-defned notion of neural network would also foster collaboration,\n",
      "as it would greatly simplify sharing models as precise mathematical quantities\n",
      "rather than diferentiable code written in a particular framework.\n",
      "Aim.\n",
      "Our ambition is to establish a unifed framework for deep learning, in\n",
      "which deep feedforward and recurrent neural networks, with or without short-\n",
      "cut connections, are defned in terms of a unique layer, which we will refer to\n",
      "as a parametric machine. This approach allows for extremely simplifed fows\n",
      "for designing neural architectures, where a small set of hyperparameters deter-\n",
      "mines the whole architecture. By virtue of their precise mathematical defnition,\n",
      "parametric machines will be language-agnostic and independent from automatic\n",
      "diferentiation engines. Computational efciency, in particular in terms of ef-\n",
      "cient gradient computation, is granted by the mathematical framework.\n",
      "Contributions.\n",
      "The theoretical framework of parametric machines unifes\n",
      "seemingly disparate architectures, designed for structured or unstructured data,\n",
      "with or without recurrent or shortcut connections. We provide theorems ensur-\n",
      "2\n",
      "ing that 1. under the assumption of fnite depth, the output of a machine can\n",
      "be computed efciently; 2. complex architectures with shortcuts can be built by\n",
      "adding together machines of depth one, thus generalizing neural networks at\n",
      "any level of granularity (neuron, layer, or entire network); 3. backpropagating\n",
      "from output to input space is again a machine computation and has a computa-\n",
      "tional cost comparable to the forward pass. In addition to the theoretical frame-\n",
      "work, we implement the input-output computations of parametric machines, as\n",
      "well as their derivatives, in the Julia programming language [6] (both on CPU\n",
      "and on GPU). Each algorithm can be used both as standalone or layer of a clas-\n",
      "sical neural network architecture.\n",
      "Structure.\n",
      "Section 2 introduces the abstract notion of machine, as well as its\n",
      "theoretical properties. In section 2.1, we defne the machine equation and the\n",
      "corresponding resolvent. There, we establish the link with deep neural networks\n",
      "and backpropagation, seen as machines on a global normed vector space. Sec-\n",
      "tion 2.2 discusses under what conditions the machine equation can be solved\n",
      "efciently, whereas in section 2.3 we discuss how to combine machines under\n",
      "suitable independence assumptions. The theoretical framework is completed in\n",
      "section 2.4, where we introduce the notion of parametric machine and discuss e\n",
      "xplicitly how to diferentiate its output with respect to the input and to the\n",
      "parameters. Section 3 is devoted to practical applications. There, we discuss\n",
      "in detail an implementation of machines that extends classical dense, convolu-\n",
      "tional, and recurrent networks with a rich shortcut structure.\n",
      "2\n",
      "Machines\n",
      "We start by setting the mathematical foundations for the study of machines.\n",
      "In order to retain two key notions that are pervasive in deep learning—linearity\n",
      "and diferentiability—we choose to work with normed vector spaces and Fréchet\n",
      "derivatives. We then proceed to build network-like architectures starting from\n",
      "continuously diferentiable maps of normed vector spaces. We refer the reader\n",
      "to appendix A for relevant defnitions and facts concerning diferentiability in\n",
      "the sense of Fréchet.\n",
      "The key intuition is that a neural network can be considered as an endo-\n",
      "function f : X → X on a space of global functions X (defned on all neurons\n",
      "on all layers). We will show that this viewpoint allows us to recover classical\n",
      "neural networks with arbitrarily complex shortcut connections. In particular,\n",
      "the forward pass of a neural network corresponds to computing the inverse of\n",
      "3\n",
      "the mapping id−f. We explore under what conditions on f, the mapping id−f\n",
      "is invertible, and provide practical strategies for computing it and its derivative.\n",
      "This meshes well with the recent trend of deep equilibrium models [1]. There,\n",
      "the output of the network is defned implicitly, as the solution of a fxed-point\n",
      "problem. Under some assumptions, such problems have a unique solution that\n",
      "can be found efciently [33]. Furthermore, the implicit function theorem can\n",
      "be used to compute the derivative of the output with respect to the input and\n",
      "parameters [11]. While the overarching formalism is similar, here we choose a\n",
      "diferent set of fxed-point problems, based on an algebraic condition which gen-\n",
      "eralizes classical feedforward architectures and does not compromise on com-\n",
      "putational efciency. We explored in [29] a frst version of the framework. Here,\n",
      "we develop a much more streamlined approach, which does not rely explicitly\n",
      "on category theory. Instead, we ground the framework in functional analysis.\n",
      "This perspective allows us to reason about automatic diferentiation and devise\n",
      "efcient algorithms for the reverse pass.\n",
      "2.1\n",
      "Resolvent\n",
      "We start by formalizing how, in the classical deep learning framework, difer-\n",
      "ent layers are combined to form a network. Intuitively, function composition\n",
      "appears to be the natural operation to do so. A sequence of layers\n",
      "X0\n",
      "l1−→ X1\n",
      "l2−→ . . . Xd−1\n",
      "ld−→ Xd\n",
      "is composed into a map X0 → Xd. We denote composition by juxtaposing\n",
      "functions:\n",
      "ldld−1 . . . l2l1 : X0 → Xd.\n",
      "However, this intuition breaks down in the case of shortcut connections or more\n",
      "complex, non-sequential architectures.\n",
      "From a mathematical perspective, a natural alternative is to consider a global\n",
      "space X = Ld\n",
      "i=0 Xi, and the global endofunction\n",
      "f =\n",
      "d\n",
      "X\n",
      "i=1\n",
      "li ∈ C1(X, X).\n",
      "(1)\n",
      "What remains to be understood is the relationship between the function f and\n",
      "the layer composition ldld−1 . . . l2l1. To clarify this relationship, we assume that\n",
      "the output of the network is the entire space X, and not only the output of the\n",
      "4\n",
      "last layer, Xd. Let the input function be the continuously diferentiable inclu-\n",
      "sion map g ∈ C1(X0, X). The map g embeds the input data into an augmented\n",
      "space, which encompasses input, hidden layers, and output. The network trans-\n",
      "forms the input map g into an output map h ∈ C1(X0, X). From a practical\n",
      "perspective, h computes the activation values of all the layers and stores not\n",
      "only the fnal result, but also all the activations of the intermediate layers.\n",
      "The key observation, on which our framework is based, is that f (the sum of\n",
      "all layers, as in eq. (1)) and g (the input function) alone are sufcient to determine\n",
      "h (the output function). Indeed, h is the only map in C1(X0, X) that respects\n",
      "the following property:\n",
      "h = g + fh.\n",
      "(2)\n",
      "In summary, we will use eq. (1) to recover neural networks as particular cases\n",
      "of our framework. There, composition of layers is replaced by their sum. Layers\n",
      "are no longer required to be sequential, but they must obey a weaker condition\n",
      "of independence, which will be discussed in detail in section 2.3. Indeed, eq. (2)\n",
      "holds also in the presence of shortcut connections, or more complex architec-\n",
      "tures such as UNet [16] (see fg. 2 for a worked example). The existence of a\n",
      "unique solution to eq. (2) for any choice of input function g is the minimum\n",
      "requirement to ensure a well-defned input-output mapping for general archi-\n",
      "tectures. It will be the defning property of a machine, our generalization of a\n",
      "feedforward deep neural network.\n",
      "Defnition 1. Let X be a normed vector space. Let k ∈ N∪{∞}. An endofunction\n",
      "f ∈ Ck(X, X) is a k-diferentiable machine if, for all normed vector space X0 and\n",
      "for all map g ∈ Ck(X0, X), there exists a unique map h ∈ Ck(X0, X) such that\n",
      "eq. (2) holds. We refer to X0 and X as input space and machine space, respectively.\n",
      "We refer to eq. (2) as the machine equation.\n",
      "In the remainder we assume and shall use k = 1, in other words machines\n",
      "are 1-diferentiable, to allow for backpropagation. However, k-diferentiable ma-\n",
      "chines, with k > 1, could be used to perform gradient-based hyperparameter\n",
      "optimization, as discussed in [3, 18]. The results shown here for k = 1 can be\n",
      "adapted in a straightforward way to k > 1.\n",
      "Defnition 1 and eq. (2) describe the link between the input function g and\n",
      "the output function h. By a simple algebraic manipulation, we can see that eq. (2)\n",
      "is equivalent to\n",
      "(id − f)h = g.\n",
      "In other words, f is a machine if and only the composition with id − f induces\n",
      "a bijection C1(X0, X)\n",
      "∼−→ C1(X0, X) for all normed vector space X0. It is a\n",
      "5\n",
      "general fact that this only happens whenever id − f is an isomorphism, as will\n",
      "be shown in the following proposition. This will allow us to prove that a given\n",
      "function f is a machine by explicitly constructing an inverse of id − f.\n",
      "Proposition 1. Let X be a normed vector space. f ∈ C1(X, X) is a machine if\n",
      "and only if id − f is an isomorphism. Whenever that is the case, the resolvent of\n",
      "f is the mapping\n",
      "Rf = (id − f)−1.\n",
      "(3)\n",
      "Then, h = g + fh if and only if h = Rfg.\n",
      "Proof. Let us assume that f is a machine. Let g = id and h be such that h =\n",
      "g + fh. Then, (id − f)h = id, that is to say id − f has a right inverse h. Let\n",
      "h1, h2 be such that (id − f)h1 = (id − f)h2. Let g = (id − f)h1 = (id − f)h2.\n",
      "Then,\n",
      "h1 = g + fh1\n",
      "and\n",
      "h2 = g + fh2,\n",
      "hence h1 = h2, therefore id − f is injective. As\n",
      "(id − f)h(id − f) = id − f\n",
      "and id − f is injective, it follows that h(id − f) = id, so id − f is necessarily an\n",
      "isomorphism. Conversely, let us assume that id − f is an isomorphism. Then,\n",
      "for all normed vector space X0 and for all g, h ∈ C1(X0, X),\n",
      "h = g + fh if and only if h = (id − f)−1g = Rfg.\n",
      "Thanks to proposition 1, it follows that the derivative of a machine, as well\n",
      "as its dual, are also machines. This will be relevant in the following sections, to\n",
      "perform parameter optimization on machines.\n",
      "Proposition 2. Let f ∈ C1(X, X) be a machine Let x0 ∈ X. Then, the derivative\n",
      "Df(x0)—a bounded linear endofunction in B(X, X)—and its dual (Df(x0))∗ ∈\n",
      "B(X∗, X∗) are machines, with resolvents\n",
      "RDf(x0) = DRf(x0)\n",
      "and\n",
      "R(Df(x0))∗ = (DRf(x0))∗ ,\n",
      "respectively.\n",
      "6\n",
      "Proof. By diferentiating eq. (3), it follows that\n",
      "DRf(x0) = (id − Df(x0))−1 ,\n",
      "(4)\n",
      "hence Df(x0) is a machine with resolvent DRf(x0). By taking the duals in\n",
      "eq. (4), it follows that\n",
      "(DRf(x0))∗ = (id − Df(x0)∗)−1 ,\n",
      "hence (Df(x0))∗ is a machine with resolvent (DRf(x0))∗.\n",
      "Examples\n",
      "Standard sequential neural networks are machines. Let us consider a product\n",
      "of normed vector spaces X = Ld\n",
      "i=0 Xi, and, for each i ∈ {1, . . . , d}, a map\n",
      "li ∈ C1(Xi−1, Xi). This is analogous to a sequential neural network. Let f =\n",
      "Pd\n",
      "i=1 li. The inverse of id − f can be constructed explicitly via the following\n",
      "sequence:\n",
      "y0 = x0\n",
      "and\n",
      "yi = li(yi−1) + xi for i ∈ {1, . . . , d}.\n",
      "Then, it is straightforward to verify that\n",
      "(id − f)(y0, y1, . . . , yd) = (y0, y1 − l1(y0), . . . , yd − ld(yd−1))\n",
      "= (x0, x1, . . . , xd).\n",
      "Conversely, let us assume that x = (id − l)˜x. Then,\n",
      "(y0, y1, . . . , yd) = (x0, l1(y0) + x1, . . . , ld(yd−1) + xd)\n",
      "= (˜x0, l1(y0) + ˜x1 − l1(˜x0), . . . , ld(yd−1) + ˜xd − ld(˜xd−1)).\n",
      "By induction, for all i ∈ {0, . . . , d}, yi = ˜xi. Hence,\n",
      "Rf(x0, x1, . . . , xd) = (y0, y1, . . . , yd)\n",
      "is the inverse of id − f.\n",
      "Classical results in linear algebra provide us with a diferent but related class\n",
      "of examples. Let us consider a bounded linear operator f ∈ B(X, X). If f is\n",
      "nilpotent, that is to say, there exists n ∈ N such that f n = 0, then f is a machine.\n",
      "The resolvent can be constructed explicitly as\n",
      "(id − f)−1 = id + f + f 2 + · · · + f n−1.\n",
      "7\n",
      "The sequential neural network and nilpotent operator examples have some\n",
      "overlap: whenever all layers li are linear, f = Pd\n",
      "i=1 li is a linear nilpotent op-\n",
      "erator. However, in general they are distinct: neural networks can be nonlin-\n",
      "ear and nilpotent operators can have more complex structures (corresponding\n",
      "to shortcut connections). The goal of the next section is to discuss a common\n",
      "generalization—machines of fnite depth.\n",
      "2.2\n",
      "Depth\n",
      "We noted in section 2.1 that nilpotent continuous linear operators and sequential\n",
      "neural networks are machines, whose resolvents can be constructed explicitly.\n",
      "The same holds true for a more general class of endofunctions, namely endo-\n",
      "functions of fnite depth. In this section, we will give a precise defnition of depth\n",
      "and show a procedure to compute the resolvent of endofunctions of fnite depth.\n",
      "We follow the convention that, given a normed vector space X, a cofltration is\n",
      "a sequence of quotients\n",
      "X/Vi → X/Vj\n",
      "for\n",
      "i ≥ j,\n",
      "where each Vi is a closed subspace of X.\n",
      "Defnition 2. Let X be a normed vector space and f ∈ C1(X, X). Let d ∈ N. A\n",
      "sequence of closed vector subspaces\n",
      "X ⊇ V0 ⊇ V1 ⊇ · · · ⊇ Vd = 0\n",
      "with associated projections πi : X → X/Vi is a depth cofltration of length d for\n",
      "f if the following conditions are verifed.\n",
      "• π0f = 0 or, equivalently, Im f ⊆ V0.\n",
      "• For all i in {1, . . . , d}, there exists ˜fi ∈ C1(X/Vi−1, X/Vi) such that\n",
      "πif = ˜fiπi−1.\n",
      "The depth of f is the length of its shortest depth cofltration, if any exists, and ∞\n",
      "otherwise.\n",
      "Remark 1. Even though it is not required that V0 = span(Im f), it is always\n",
      "possible, given a depth cofltration V0, . . . , Vn, to construct a new depth cofltration\n",
      "˜Vi = Vi ∩ span(Im f).\n",
      "8\n",
      "Since span(Im f) ⊆ V0, then ˜V0 = span(Im f). This will be useful when combin-\n",
      "ing depth cofltrations to build depth cofltrations of more complex endofunctions,\n",
      "as in theorem 2.\n",
      "Proposition 3. Let f ∈ C1(X, X) be a machine. A sequence of closed vector\n",
      "subspaces\n",
      "X ⊇ V0 ⊇ V1 ⊇ · · · ⊇ Vd = 0\n",
      "is a depth cofltration for f if and only if it is a depth cofltration for Df(x0) for\n",
      "all x0 ∈ X. Whenever that is the case,\n",
      "X∗ ⊇ (X/Vd−1)∗ ⊇ · · · ⊇ (X/V0)∗ ⊇ (X/X)∗ = 0\n",
      "is a depth cofltration for (Df(x0))∗ for all x0 ∈ X.\n",
      "Proof. The claim concerning the diferential machine follows by proposition 5\n",
      "in appendix A.\n",
      "In simple cases, depth cofltrations can be computed directly. For instance,\n",
      "if f is linear and continuous, then\n",
      "X ⊇ ker f d ⊇ · · · ⊇ ker f ⊇ ker f 0 = 0\n",
      "is a depth cofltration for f if f d+1 = 0. Conversely, if a continuous linear\n",
      "operator f admits a depth cofltration of length d, then necessarily f d+1 = 0.\n",
      "Hence, a continuous linear operator has fnite depth if and only if it is nilpotent.\n",
      "Sequential neural networks are another example of endofunction of fnite\n",
      "depth. Let us consider a sequential architecture\n",
      "X0\n",
      "l1−→ X1\n",
      "l2−→ . . . Xd−1\n",
      "ld−→ Xd\n",
      "and\n",
      "l =\n",
      "n\n",
      "X\n",
      "i=1\n",
      "li ∈ C1(X, X), where X = X0 ⊕ · · · ⊕ Xd.\n",
      "Naturally Vi = Xi+1 + · · · + Xd defnes a depth cofltration for l. A similar\n",
      "result holds for acyclic neural architectures with arbitrarily complex shortcuts.\n",
      "However, proving that directly is nontrivial: it will become much more straight-\n",
      "forward with the tools developed in section 2.3. For now, we will simply assume\n",
      "that a given endofunction f has fnite depth, and we will show how to compute\n",
      "its resolvent.\n",
      "9\n",
      "Defnition 3. Let f ∈ C1(X, X) and g ∈ C1(X0, X). Let d ∈ N and let\n",
      "X ⊇ V0 ⊇ V1 ⊇ · · · ⊇ Vd = 0\n",
      "be a depth cofltration. Its associated depth sequence is defned as follows:\n",
      "˜h0 = π0g\n",
      "and\n",
      "˜hi = πig + ˜fi˜hi−1 for i ∈ {1, . . . , d},\n",
      "where for all i ∈ {1, . . . , d}, ˜hi ∈ C1(X0, X/Vi). A sequence {h0, . . . , hd} ⊆\n",
      "C1(X0, X) is a lifted depth sequence if πihi = ˜hi for all i ∈ {0, . . . , d}.\n",
      "In other words, a depth sequence is a sequence of functions that approximate\n",
      "more and more accurately a solution of the machine equation, as we will prove in\n",
      "the following theorem. In general, the depth sequence can be lifted in diferent\n",
      "ways, which correspond to algorithms to solve the machine equation of diferent\n",
      "computational efciency, as shown in fg. 1.\n",
      "Proposition 4. Let φ ∈ C1(X0, V0). The sequence\n",
      "hφ\n",
      "0 = g + φ\n",
      "and\n",
      "hφ\n",
      "i = g + fhφ\n",
      "i−1 for i ∈ {1, . . . , d}\n",
      "is a lifted depth sequence.\n",
      "Proof. For i = 0, π0hφ\n",
      "0 = π0(g + φ) = π0g = ˜h0. If πi−1hφ\n",
      "i−1 = ˜hφ\n",
      "i−1, then\n",
      "πihφ\n",
      "i = πi(g + fhφ\n",
      "i−1) = πig + πifhφ\n",
      "i−1 = πig + ˜fiπi−1hφ\n",
      "i−1 = πg + ˜fi˜hi−1 = ˜hi,\n",
      "hence the claim follows by induction.\n",
      "Theorem 1. Let us assume that f ∈ C1(X, X) admits a depth cofltration of\n",
      "length d. Then, f is a machine. Furthermore, let us consider g ∈ C1(X0, X), and\n",
      "let ˜h be its depth sequence. Then, ˜hd = g + f˜hd.\n",
      "Proof. Let hφ\n",
      "i be as in proposition 4. Then, h0\n",
      "d = hfg\n",
      "d = h0\n",
      "d+1 = fg + h0\n",
      "d, hence\n",
      "h0\n",
      "d = ˜hd solves the machine equation. To see uniqueness, let h be a solution to\n",
      "h = fg + h. Then, for all i ∈ {0, . . . , d}, h = hfh\n",
      "i , hence h0\n",
      "d = hfh\n",
      "d = h.\n",
      "2.3\n",
      "Composability\n",
      "Here, we develop the notion of machine independence, which will be crucial for\n",
      "composability, as it will allow us to create complex machines as a sum of simpler\n",
      "ones. In particular, we will show that deep neural networks can be decomposed\n",
      "as sum of layers and are therefore machines of fnite depth.\n",
      "10\n",
      "0\n",
      "5\n",
      "10\n",
      "node\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "5\n",
      "10\n",
      "activation value\n",
      "optimized sequence\n",
      "expensive sequence\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "step\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "Figure 1: Diferent sequences to solve a linear machine with shortcuts on\n",
      "5 nodes. The left column shows the connectivity graph, whereas the right col-\n",
      "umn describes the accumulated activation value on each node for a given input.\n",
      "For visual simplicity, we take positive connectivity matrix and input values, so\n",
      "that all updates are positive and can be represented concisely in a stacked bar\n",
      "plot. Two possible approaches to solve the machine equation are exemplifed.\n",
      "The top row represents an efcient strategy: at step i we update only the i-th\n",
      "node, as implied by color and orientation of the edges. In the bottom row, at the\n",
      "i-th step we evaluate fhi + g. This is inefcient, as some connections need to\n",
      "be recomputed several times, as encoded by the line width of the edges of the\n",
      "bottom graph. The optimized sequence avoids this inefciency by deferring the\n",
      "computation of each connection until the input value is fully determined.\n",
      "11\n",
      "Defnition 4. Let X be a normed vector space. Let f1, f2 ∈ C1(X, X). We say\n",
      "that f1 does not depend on f2 if, for all x1, x2 ∈ X, and for all λ ∈ R, the\n",
      "following holds:\n",
      "f1(x1 + λf2(x2)) = f1(x1).\n",
      "(5)\n",
      "Otherwise, we say that f1 depends on f2.\n",
      "Defnition 4 is quite useful to compute resolvents. For instance, f does not\n",
      "depend on itself if and only if it has depth at most 1, in which case it is a machine,\n",
      "and its resolvent can be computed via Rf = id + f. Furthermore, by combining\n",
      "machines of fnite depth with appropriate independence conditions, we again\n",
      "obtain machines of fnite depth.\n",
      "If f1 is linear, then f1 does not depend on f2 if and only if f1f2 = 0, but in\n",
      "general the two notions are distinct. For instance, the following pair of functions\n",
      "f1(x) = x − 3\n",
      "and\n",
      "f2(x) = 3\n",
      "respects f1f2 = 0, but f1 depends on f2 as x − 3λ ̸= x for λ ̸= 0.\n",
      "It follows from proposition 5 that defnition 4 has some alternative formu-\n",
      "lations. f1 does not depend on f2 if and only if it factors through the following\n",
      "quotient:\n",
      "X\n",
      "X\n",
      "X/span(Im f2)\n",
      "π\n",
      "f1\n",
      "That is equivalent to requiring that at all points the diferential of f1 factors via\n",
      "π, that is to say\n",
      "(Df1(x1))f2(x2) = 0 for all x1, x2 ∈ X.\n",
      "(6)\n",
      "Given f1, f2 ∈ C1(X, X), the sets\n",
      "{f ∈ C1(X, X) | D(f1(x1))f(x2) = 0 for all x1, x2 ∈ X}\n",
      "and\n",
      "{f ∈ C1(X, X) | D(f(x1))f2(x2) = 0 for all x1, x2 ∈ X}\n",
      "are vector spaces, as they are the intersection of kernels of linear operators. In\n",
      "other words, if f1 does not depend on f2 and ˆf2, then it also does not depend on\n",
      "λf2 + ˆλ ˆf2, and if f1 and ˆf1 do not depend on f2, then neither does λf1 + ˆλ ˆf1.\n",
      "12\n",
      "Theorem 2. Let f1, f2 be machines, of depth d1, d2 respectively, such that f1 does\n",
      "not depend on f2. Then f1+f2 is also a machine of depth d ≤ d1+d2 and Rf1+f2 =\n",
      "Rf2Rf1. If furthermore f2 does not depend on f1, then Rf1+f2 = Rf1 + Rf2 − id\n",
      "and d ≤ max(d1, d2).\n",
      "Proof. By proposition 1 and eq. (5), f1 + f2 is a machine:\n",
      "(id − f1)(id − f2) = (id − f1 − f2),\n",
      "(7)\n",
      "so (id−f1−f2) is an isomorphism (composition of isomorphisms). Equation (7)\n",
      "also determines the resolvent:\n",
      "Rf1+f2 = (id − f1 − f2)−1 = (id − f2)−1(id − f1)−1 = Rf2Rf1.\n",
      "Moreover, if f2 does not depend on f1, then\n",
      "f1(Rf1 + Rf2 − id) = f1(Rf1 + f2Rf2) = f1Rf1 = Rf1 − id,\n",
      "f2(Rf1 + Rf2 − id) = f2(f1Rf1 + Rf2) = f2Rf2 = Rf2 − id.\n",
      "Hence,\n",
      "Rf1 + Rf2 − id = id + (f1 + f2)(Rf1 + Rf2 − id).\n",
      "To prove the bounds on d, we can assume that d1 and d2 are fnite, otherwise\n",
      "the claim is trivial. Let X ⊇ V 1\n",
      "0 ⊇ · · · ⊇ V 1\n",
      "d1 = 0 and X ⊇ V 2\n",
      "0 ⊇ · · · ⊇ V 2\n",
      "d2 = 0\n",
      "be depth cofltrations of minimal length for f1 and f2 respectively. By remark 1,\n",
      "we can choose them such that\n",
      "V 1\n",
      "0 = span(Im f1)\n",
      "and\n",
      "V 2\n",
      "0 = span(Im f2).\n",
      "If f1 does not depend on f2, then\n",
      "X ⊇ V 1\n",
      "0 + V 2\n",
      "0 ⊇ · · · ⊇ V 1\n",
      "d1−1 + V 2\n",
      "0 ⊇ V 2\n",
      "0 ⊇ · · · ⊇ V 2\n",
      "d2 = 0\n",
      "is a depth cofltration of length d1 + d2 for f1 + f2. If also f2 does not depend\n",
      "on f1, then we can set d = max(d1, d2) and defne\n",
      "X ⊇ V 1\n",
      "0 + V 2\n",
      "0 ⊇ V 1\n",
      "1 + V 2\n",
      "1 · · · ⊇ V 1\n",
      "d−1 + V 2\n",
      "d−1 ⊇ V 1\n",
      "d + V 2\n",
      "d = 0,\n",
      "where by convention V 1\n",
      "i = 0 if i > d1 and V 2\n",
      "i = 0 if i > d2.\n",
      "Remark 2. The depth inequality, that is to say if f1 does not depend on f2, then\n",
      "the depth of the sum of f1 and f2 is bounded by the sum of the depths, is a nonlinear\n",
      "equivalent of an analogous result in linear algebra. Namely, given L1, L2 nilpotent\n",
      "operators with L1L2 = 0, the sum L1 +L2 is also nilpotent, and if Ln1\n",
      "1 = Ln2\n",
      "2 = 0,\n",
      "then (L1 + L2)n1+n2−1 = 0.\n",
      "13\n",
      "X1\n",
      "X2\n",
      "X3\n",
      "X4\n",
      "X5\n",
      "X6\n",
      "X7\n",
      "X8\n",
      "f1 : X1 × X2 → X3\n",
      "f2 : X1 → X5\n",
      "f3 : X3 → X4\n",
      "f4 : X4 → X5 × X6 × X7\n",
      "f5 : X6 → X8\n",
      "Figure 2: Graphical representation of a neural network with complex\n",
      "shortcuts as sum of machines of depth 1. This graphical representation\n",
      "corresponds to the neural network mapping (x1,\n",
      "x2,\n",
      "x3,\n",
      "x4,\n",
      ". . . , x8) to\n",
      "(y1, y2, y3, y4, . . . , y8) via layers {f1, . . . , f5}.\n",
      "Explicitly, output values are computed as follows:\n",
      "y1 = x1\n",
      "y2 = x2\n",
      "y3 = f1(x1, x2) + x3\n",
      "y4 = f3(f1(x1, x2) + x3) + x4\n",
      "y5 = f2(x1) + πX5f4(f3(f1(x1, x2) + x3) + x4) + x5\n",
      "y6 = πX6f4(f3(f1(x1, x2) + x3) + x4) + x6\n",
      "y7 = πX7f4(f3(f1(x1, x2) + x3) + x4) + x7\n",
      "y8 = f5(πX6f4(f3(f1(x1, x2) + x3) + x4) + x6) + x8\n",
      "14\n",
      "A natural notion of architecture with shortcuts follows from theorem 2. Let\n",
      "f1, . . . , fn be such that fi does not depend on fj if i ≤ j. Then each fi has depth\n",
      "at most 1, hence f = Pn\n",
      "i=1 fi has depth at most n, by theorem 2. Indeed, f1 +\n",
      "· · ·+fi−1 does not depend on fi, as can be verifed for each addend individually\n",
      "thanks to eq. (6), hence by induction f1 + · · · + fi has depth at most i. Then, f\n",
      "is a machine of depth at most n, whose resolvent can be computed as\n",
      "Rfn · · · Rf1g = (id + fn) · · · (id + f1)g.\n",
      "In practice, this corresponds to the lifted depth sequence\n",
      "˜h0 = g\n",
      "and\n",
      "˜hi+1 = ˜hi + fi˜hi.\n",
      "This strategy can be applied to acyclic architectures with arbitrarily complex\n",
      "shortcuts, as illustrated in fg. 2. The architecture described there has depth at\n",
      "most 4, as the endofunctions f1, f2 +f3, f4, f5 all have depth at most 1, and each\n",
      "of them does not depend on the following ones.\n",
      "More generally, theorem 2 establishes a clear link between sums of indepen-\n",
      "dent machines and compositions of layers in classical feedforward neural net-\n",
      "works. The independence condition determines the order in which machines\n",
      "should be concatenated, even in the presence of complex shortcut connections.\n",
      "Furthermore, if the initial building blocks all have fnite depth, then so does the\n",
      "sum. Thus, we can compute the machine’s resolvent efciently. As a conse-\n",
      "quence, machines of fnite depth are a practically computable generalization of\n",
      "deep neural networks and nilpotent operators.\n",
      "2.4\n",
      "Optimization\n",
      "The ability to minimize an error function is crucial in machine learning appli-\n",
      "cations. This section is devoted to translating classical backpropagation-based\n",
      "optimization to our framework. Given the input map g: X0 → X and a loss\n",
      "function L: X → R, we wish to fnd f : X → X such that the composition Lh\n",
      "is minimized. To constrain the space of possible endofunctions (architectures\n",
      "and weights), we restrict the choice of f to a smoothly parameterized family of\n",
      "functions fp, where p varies within a parameter space P.\n",
      "Parametric machines.\n",
      "Let P be a normed vector space of parameters. A\n",
      "parametric machine is a C1 family of machines f(p, x): P × X → X such that,\n",
      "given a C1 family of input functions g(p, x0), the family of resolvents h(p, x0)\n",
      "15\n",
      "is also jointly C1 in both arguments. We call f a parametric machine, with pa-\n",
      "rameter space P. Whenever f is a parametric machine, we denote by Rf its\n",
      "parametric resolvent, that is the only function in C1(P × X, X) such that\n",
      "Rf(p, x0) = x0 + f(p, Rf(p, x0)).\n",
      "In practical applications, we are interested in computing the partial deriva-\n",
      "tives of the parametric resolvent function Rf with respect to the parameters and\n",
      "the inputs. This can be done using the derivatives of f and a resolvent computa-\n",
      "tion. Therefore, the structure and cost of the backward pass (backpropagation)\n",
      "are comparable to those of the forward pass. We recall that the backward pass\n",
      "is the computation of the dual operator of the derivative of the forward pass.\n",
      "Theorem 3. Let f(p, x) be a parametric machine. Let Rf denote the parametric\n",
      "resolvent mapping\n",
      "x = Rf(p, x0).\n",
      "Then, the following equations hold:\n",
      "∂Rf\n",
      "∂x0\n",
      "= R ∂f\n",
      "∂x\n",
      "and\n",
      "∂Rf\n",
      "∂p = ∂Rf\n",
      "∂x0\n",
      "∂f\n",
      "∂p.\n",
      "(8)\n",
      "Analogously, by considering the dual of each operator,\n",
      "\u0012∂Rf\n",
      "∂x0\n",
      "\u0013∗\n",
      "=\n",
      "\u0010\n",
      "R ∂f\n",
      "∂x\n",
      "\u0011∗\n",
      "and\n",
      "\u0012∂Rf\n",
      "∂p\n",
      "\u0013∗\n",
      "=\n",
      "\u0012∂f\n",
      "∂p\n",
      "\u0013∗ \u0012∂Rf\n",
      "∂x0\n",
      "\u0013∗\n",
      ".\n",
      "(9)\n",
      "In other words,\n",
      "• the partial derivative of Rf with respect to the inputs can be obtained via a\n",
      "resolvent computation, and\n",
      "• the partial derivative of Rf with respect to the parameters is the composi-\n",
      "tion of the partial derivative of Rf with respect to the inputs and the partial\n",
      "derivative of f with respect to the parameters.\n",
      "Proof. We can diferentiate Rf with respect to p and x0 by diferentiating the\n",
      "machine equation x = x0 + f(p, x). Explicitly,\n",
      "∂Rf\n",
      "∂x0\n",
      "=\n",
      "\u0012\n",
      "id − ∂f\n",
      "∂x\n",
      "\u0013−1\n",
      "= R ∂f\n",
      "∂x and ∂Rf\n",
      "∂p =\n",
      "\u0012\n",
      "id − ∂f\n",
      "∂x\n",
      "\u0013−1 ∂f\n",
      "∂p = ∂Rf\n",
      "∂x0\n",
      "∂f\n",
      "∂p.\n",
      "Equation (9) follows from eq. (8) by duality.\n",
      "16\n",
      "The relevance of theorem 3 is twofold. On the one hand, it determines a\n",
      "practical approach to backpropagation for general parametric machines. Initially\n",
      "the resolvent of\n",
      "\u0000 ∂f\n",
      "∂x\n",
      "\u0001∗ is computed on the gradient of the loss function L. Then,\n",
      "the result is backpropagated to the parameters. In symbols,\n",
      "∂L(Rf(p, x0))\n",
      "∂p\n",
      "=\n",
      "\u0012∂f\n",
      "∂p\n",
      "\u0013∗ \u0012∂Rf\n",
      "∂x0\n",
      "\u0013∗\n",
      "DL(Rf(p, x0)).\n",
      "The gradient DL(x), where x = Rf(p, x0), linearly maps tangent vectors of\n",
      "X to scalars and is therefore a cotangent vector of X. Indeed, the dual ma-\n",
      "chine\n",
      "\u0000 ∂f\n",
      "∂x\n",
      "\u0001∗ is an endofunction of the cotangent space of X. On the other hand,\n",
      "theorem 3 guarantees that in a broad class of practical cases the computational\n",
      "complexity of the backward pass is comparable to the computational complexity\n",
      "of the forward pass. We will show this practically in the following section.\n",
      "3\n",
      "Implementation and performance\n",
      "In this section, we shall analyze several standard and non-standard architectures\n",
      "in the machine framework, provide a general implementation strategy, and dis-\n",
      "cuss memory usage and performance for both forward and backward pass. We\n",
      "consider a broad class of examples where f has both a linear component wp\n",
      "(parametrized by p) and a nonlinear component σ. Diferent choices of w will\n",
      "correspond to diferent architecture (multi-layer perceptron, convolutional neu-\n",
      "ral network, recurrent neural network) with or without shortcuts.\n",
      "We split the space X as a direct sum X = Y ⊕ Z, i.e., x = (y, z), where y\n",
      "and z correspond to values before and after the nonlinear activation function,\n",
      "respectively. Hence, we write fp = wp + σ, with\n",
      "σ: Y → Z\n",
      "and\n",
      "wp : Z → Y.\n",
      "The machine equation\n",
      "x = fp(x) + x0\n",
      "can be written as a simple system of two equations:\n",
      "y = wpz + y0\n",
      "and\n",
      "z = σ(y) + z0.\n",
      "Given cotangent vectors u0 ∈ Z∗, v0 ∈ Y ∗ (which are themselves computed by\n",
      "backpropagating the loss on the machine output) we can run the following dual\n",
      "machine:\n",
      "u = w∗\n",
      "pv + u0\n",
      "and\n",
      "v = (Dσ(y))∗u + v0.\n",
      "17\n",
      "Then, eq. (9) boils down to the following rule to backpropagate (v0, u0) both to\n",
      "the input and the parameter space.\n",
      "\u0012 ∂x\n",
      "∂x0\n",
      "\u0013∗\n",
      "(v0, u0) = (v, u),\n",
      "and\n",
      "\u0012∂x\n",
      "∂p\n",
      "\u0013∗\n",
      "(v0, u0) =\n",
      "\u0012∂wp\n",
      "∂p\n",
      "\u0013∗\n",
      "v.\n",
      "In practical cases, the computation of the dual machine has not only the same\n",
      "structure, but also the same computational complexity of the forward pass. In\n",
      "particular, in the cases we will analyze, the global linear operator wp ∈ B(Y, Z)\n",
      "will be either a fully-connected or a convolutional layer, hence the dual w∗\n",
      "p would\n",
      "be a fully-connected or a transpose convolutional layer respectively, with com-\n",
      "parable computational cost, as shown practically in fg. 3 (see table 1 for the\n",
      "exact numbers). In our applications, the nonlinearity σ will be pointwise, hence\n",
      "the derivative Dσ(x) can be computed pointwise, again with comparable com-\n",
      "putational cost to the computation of σ. Naturally, for σ to act pointwise, we\n",
      "require that Y ≃ Z ≃ RI for some index set I.\n",
      "The frst obstacle in defning a machine of the type wp + σ is practical. How\n",
      "should one select a linear operator wp and a pointwise nonlinearity σ, under the\n",
      "constraint that wp + σ is a machine of fnite depth? We adopt a general strat-\n",
      "egy, starting from classical existing layers and partitions on index spaces. We\n",
      "take lp to be a linear operator (in practice, a convolutional or fully connected\n",
      "layer). We consider a partition I = Fn\n",
      "i=0 Ii of the underlying index set I. For\n",
      "i ∈ {0, . . . , n}, let πY\n",
      "i , πZ\n",
      "i be the projection from Y or Z to the subspace corre-\n",
      "sponding to I0 ⊔ · · · ⊔ Ii. We can defne the linear component of the machine as\n",
      "follows:\n",
      "wp =\n",
      "n\n",
      "X\n",
      "i=1\n",
      "\u0000πY\n",
      "i − πY\n",
      "i−1\n",
      "\u0001\n",
      "lpπZ\n",
      "i−1,\n",
      "that is to say, it is a modifed version of lp such that outputs in index subsets\n",
      "depend only on inputs in previous index subsets. It is straightforward to verify\n",
      "that\n",
      "X = Y ⊕ Z ⊇ ker πY\n",
      "0 + Z\n",
      "⊇ ker πY\n",
      "0 + ker πZ\n",
      "0\n",
      "⊇ ker πY\n",
      "1 + ker πZ\n",
      "0\n",
      "⊇ ker πY\n",
      "1 + ker πZ\n",
      "1\n",
      "...\n",
      "⊇ ker πY\n",
      "n + ker πY\n",
      "n = 0\n",
      "18\n",
      "is a depth cofltration for wp + σ, hence wp + σ is a machine of depth at most\n",
      "2n + 1.\n",
      "Generalized multi-layer perceptron\n",
      "Let us consider a generalization of the multi-layer perceptron in our frame-\n",
      "work. Let x[c] (a point in machine space) be a tensor with one index, where\n",
      "c ∈ {1, . . . , nc}. Let I0, . . . , In be a partition of {1, . . . , nc}. We adapt the no-\n",
      "tation of the previous section: whenever possible, capital letters denote tensors\n",
      "corresponding to linear operators in lower case. Let L[c2, c1] be a tensor with\n",
      "two indices c1, c2 ∈ {1, . . . , nc}, let\n",
      "W =\n",
      "n\n",
      "X\n",
      "i=1\n",
      "\u0000πY\n",
      "i − πY\n",
      "i−1\n",
      "\u0001\n",
      "LπZ\n",
      "i−1,\n",
      "and let σ a pointwise nonlinearity. We consider the machine equation\n",
      "z = σ(y) + z0,\n",
      "(10)\n",
      "y = Wz + y0.\n",
      "(11)\n",
      "The backward pass can be computed via the dual machine computation\n",
      "v = σ′(y) ⊙ u + v0,\n",
      "(12)\n",
      "u = W ∗v + u0,\n",
      "(13)\n",
      "where σ′ is the derivative of σ and ⊙ is the Hadamard (elementwise) product,\n",
      "and the equations\n",
      "Q =\n",
      "n\n",
      "X\n",
      "i=1\n",
      "\u0000πY\n",
      "i − πY\n",
      "i−1\n",
      "\u0001\n",
      "vz∗πX\n",
      "i−1,\n",
      "(14)\n",
      "where Q represents the cotangent vector (v0, u0) backpropagated to the param-\n",
      "eters W. Equations (10) to (14) can be solved efciently following the procedure\n",
      "described in algorithm 1. We describe the procedure exclusively for generalized\n",
      "multi-layer perceptrons, but the equivariant case (convolutional and recurrent\n",
      "neural networks) is entirely analogous.\n",
      "Equivariant architectures\n",
      "We include under the broad term equivariant architectures [4] all machines\n",
      "whose underlying linear operator wp is translation-equivariant—a shift in the\n",
      "19\n",
      "10⁰\n",
      "10¹\n",
      "10²\n",
      "10³\n",
      "10⁴\n",
      "dense\n",
      "convolution\n",
      "recurrent\n",
      "10⁰\n",
      "10¹\n",
      "10²\n",
      "10³\n",
      "10⁴\n",
      "dense\n",
      "convolution\n",
      "recurrent\n",
      "runtime (μs)\n",
      "small size\n",
      "medium size\n",
      "machine\n",
      "CPU\n",
      "GPU\n",
      "pass\n",
      "forward\n",
      "backward\n",
      "Figure 3: Ratio of runtime of backward pass over forward pass. The run-\n",
      "times of backward and forward pass are comparable, across diferent models,\n",
      "problem sizes, and devices. The computation of the backward pass assumes that\n",
      "the forward pass has been computed already, and that its result is available. The\n",
      "backward pass denotes the backpropagation of cotangent vectors from machine\n",
      "space to input space. Backpropagating to parameter space requires an extra op-\n",
      "eration (see e.g. eq. (14) for the dense case).\n",
      "20\n",
      "Algorithm 1 Computation of non-equivariant machine.\n",
      "Forward pass:\n",
      "1: Initialize arrays y, z of size nc and value y = y0, z = z0\n",
      "2: for i = 0 to n do\n",
      "3:\n",
      "Set y[Ii] += W[Ii, :]z, eq. (11)\n",
      "4:\n",
      "Set z[Ii] += σ (y[Ii]), eq. (10)\n",
      "5: end for\n",
      "Backward pass:\n",
      "1: Initialize arrays u, v of size nc and value u = u0, v = v0\n",
      "2: for i = n to 0 do\n",
      "3:\n",
      "Set u[Ii] += (L[:, Ii])∗ v, eq. (13)\n",
      "4:\n",
      "Set v[Ii] += σ′ (y[Ii]) ⊙ u[Ii], eq. (12)\n",
      "5: end for\n",
      "6: Initialize Q = vz∗, eq. (14)\n",
      "7: Set Q[Ij, Ii] = 0, for all j ≤ i, eq. (14)\n",
      "input corresponds to a shift in the output. This includes convolutional layers\n",
      "for temporal or spatial data, as well as recurrent neural networks, if we con-\n",
      "sider the input as a time series that can be shifted forward or backward in time.\n",
      "The similarity between one-dimentional convolutional neural networks and re-\n",
      "current neural networks will become clear in the machine framework. Both\n",
      "architectures can be implemented with the same linear operator lp but diferent\n",
      "index space partitions.\n",
      "The equivariant case is entirely analogous to the non-equivariant one. We\n",
      "consider the simplest scenario: one-dimensional convolutions of stride one for,\n",
      "e.g., time series data. We consider a discrete grid with two indices\n",
      "t ∈ {1, . . . , nt},\n",
      "c ∈ {1, . . . , nc},\n",
      "referring to time and channel, respectively. Thus, the input data will be a tensor\n",
      "of two indices, y[t, c]. The convolutional kernel will be a tensor of three in-\n",
      "dices, L[τ, c1, c2], representing time lag (kernel size), input channel, and output\n",
      "channel, respectively. Let I0, . . . , In be a partition of {1, . . . , nt} × {1, . . . , nc}.\n",
      "We again denote\n",
      "W =\n",
      "n\n",
      "X\n",
      "i=1\n",
      "\u0000πY\n",
      "i − πY\n",
      "i−1\n",
      "\u0001\n",
      "LπZ\n",
      "i−1\n",
      "21\n",
      "and consider the machine equation\n",
      "z = σ(y) + z0,\n",
      "y = W ∗ z + y0.\n",
      "where ∗ denotes convolution. The backward pass can be computed via the dual\n",
      "machine computation\n",
      "v = σ′(y) ⊙ u,\n",
      "u = W ∗t v + u0,\n",
      "where ∗t denotes transposed convolution, and the equations\n",
      "ˆQ[τ, c1, c2] =\n",
      "nt\n",
      "X\n",
      "t=τ+1\n",
      "z[t − τ, c1]v[t, c2],\n",
      "Q =\n",
      "n\n",
      "X\n",
      "i=1\n",
      "\u0000πY\n",
      "i − πY\n",
      "i−1\n",
      "\u0001 ˆQπX\n",
      "i−1,\n",
      "where Q represents the cotangent vector u0 backpropagated to the parameters.\n",
      "A common generalization of convolutional and recurrent neural net-\n",
      "works.\n",
      "Specifc choices of the partition I1, . . . , In will give rise to radically\n",
      "diferent architectures. In particular, setting Ii = {1, . . . , nt} × Ji for some par-\n",
      "tition J0 ⊔ · · · ⊔ Jn = {1, . . . , nc} gives a deep convolutional network with all\n",
      "shortcuts. On the other hand, setting It,i = {t} × Ji (where It,i are sorted by\n",
      "lexicographic order of (t, i)) yields a recurrent neural network with shortcuts in\n",
      "depth and time. The dual machine procedure is then equivalent to a generaliza-\n",
      "tion of backpropagation through time in the presence of shortcuts.\n",
      "Memory usage.\n",
      "Machines’ forward and backward pass computations are im-\n",
      "plemented diferently from classical feedforward or recurrent neural networks.\n",
      "Here, we store in memory a global tensor of all units at all depths, and we update\n",
      "it in place in a blockwise fashion. This may appear memory-intensive compared\n",
      "to traditional architectures. For instance, when computing the forward pass of\n",
      "a feedforward neural network without shortcuts, the outputs of all but the most\n",
      "recently computed layer can be discarded. However, those values are needed to\n",
      "compute gradients by backpropagation and are stored in memory by the auto-\n",
      "matic diferentiation engine. Hence, machines and neural networks have com-\n",
      "parable memory usage during training.\n",
      "22\n",
      "4\n",
      "Conclusions\n",
      "We provide solid functional foundations for the study of deep neural networks.\n",
      "Borrowing ideas from functional analysis, we defne the abstract notion of ma-\n",
      "chine, whose resolvent generalizes the computation of a feedforward neural net-\n",
      "work. It is a unifed concept that encompasses several favors of manually de-\n",
      "signed neural network architectures, both equivariant (convolutional [15] and\n",
      "recurrent [31] neural networks) and non-equivariant (multilayer perceptron,\n",
      "see [23]) architectures. This approach attempts to answer a seemingly simple\n",
      "question: what are the defning features of deep neural networks? More practi-\n",
      "cally, how can a deep neural network be specifed?\n",
      "On this question, current deep learning frameworks are broadly divided in\n",
      "two camps. On the one hand, domain-specifc languages allow users to defne ar-\n",
      "chitectures by combining a selection of pre-existing layers. On the other hand, in\n",
      "the diferentiable programming framework, every code is a model, provided that\n",
      "the automatic diferentiation engine can diferentiate its output with respect to\n",
      "its parameters. Here, we aim to strike a balance between these opposite ends of\n",
      "the confgurability spectrum—domain-specifc languages versus diferentiable\n",
      "programming. This is done via a principled, mathematical notion of machine:\n",
      "an endofunction of a normed vector space respecting a simple property. A subset\n",
      "of machines, machines of fnite depth, are a computable generalization of deep\n",
      "neural networks. They are inspired by nilpotent linear operators, and indeed our\n",
      "main theorem concerning computability generalizes a classical result of linear\n",
      "algebra—the identity minus a nilpotent linear operator is invertible. The output\n",
      "of such a machine can be computed by iterating a simple sequence, whose be-\n",
      "havior is remindful of non-normal networks [12], where the global activity can\n",
      "be amplifed before converging to a stable state.\n",
      "We use a general procedure to defne several classes of machines of fnite\n",
      "depth. As a starting point, we juxtapose linear and nonlinear continuous end-\n",
      "ofunctions of a normed vector space. This alternation between linear and non-\n",
      "linear components is one of the key ingredients of the success of deep neural\n",
      "networks, as it allows one to obtain complex functions as a composition of sim-\n",
      "pler ones. The notion of composition of layers in neural networks is unfortu-\n",
      "nately ill-defned, especially in the presence of shortcut connections and non-\n",
      "sequential architectures. In the proposed machine framework, the composition\n",
      "is replaced by the sum, and thus sequentiality is replaced by the weaker no-\n",
      "tion of independence. We describe independence conditions to ensure that the\n",
      "sum of machines is again a machine, in which case we can compute its resol-\n",
      "vent (forward pass) explicitly. This may seem counterintuitive, as the sum is a\n",
      "23\n",
      "commutative operation, whereas the composition is not. However, in our frame-\n",
      "work, we can determine the order of composition of a collection of machines via\n",
      "their dependency structure, and thus compute the forward pass efciently.\n",
      "Once we have established how to compute the forward pass of a machine,\n",
      "the backward pass is entirely analogous and can be framed as a resolvent com-\n",
      "putation. This allows us to implement a backward pass computation in a time\n",
      "comparable to that of the forward pass, without resorting to automatic diferen-\n",
      "tiation engines, provided that we can compute the derivative of the pointwise\n",
      "nonlinearity, which is either explicitly available or can be obtained efciently\n",
      "with scalar forward-mode diferentiation. In practice, we show that not only\n",
      "the structure but also the runtime of the backward pass are comparable to those\n",
      "of the forward pass and do not incur in automatic diferentiation overhead [26].\n",
      "We believe that encompassing both forward and backward pass within a unifed\n",
      "computational framework can be particularly relevant in models where not only\n",
      "the output of the network, but also its derivatives are used in the forward pass,\n",
      "as for example gradient-based regularization [8, 28] or neural partial diferential\n",
      "equations [34].\n",
      "The strategy highlighted here to defne machines of fnite depth often gen-\n",
      "erates architectures with a large number of shortcut connections. Indeed, in\n",
      "the machine framework, these are more natural than purely sequential archi-\n",
      "tectures. Clearly, classical, sequential architectures can be recovered by forcing\n",
      "a subset of parameters to equal zero, thus cancelling the shortcut connections.\n",
      "However, this is only one of many possible ways of regularizing a machine. Sev-\n",
      "eral other approaches exist: setting to zero a diferent subset of parameters, as\n",
      "in the lottery ticket hypothesis [9], penalizing large diferences between adja-\n",
      "cent parameters, or, more generally, choosing a representation of the parameter\n",
      "space with an associated notion of smoothness, as in kernel methods [25]. We\n",
      "intend to investigate the relative merits of these approaches in a future work.\n",
      "Author contributions\n",
      "P.V. and M.G.B devised the project. P.V. and M.G.B developed the mathematical\n",
      "framework. P.V. and M.G.B. developed the software to implement the frame-\n",
      "work. P.V. wrote the original draft. M.G.B. reviewed and edited.\n",
      "24\n",
      "References\n",
      "[1] S. Bai, J. Z. Kolter, and V. Koltun. Deep equilibrium models. Advances in\n",
      "Neural Information Processing Systems, 32, 2019.\n",
      "[2] P. Barham and M. Isard. Machine learning systems are stuck in a rut. In\n",
      "Proceedings of the Workshop on Hot Topics in Operating Systems, pages 177–\n",
      "183, 2019.\n",
      "[3] Y. Bengio. Gradient-based optimization of hyperparameters. Neural com-\n",
      "putation, 12(8):1889–1900, 2000.\n",
      "[4] M. G. Bergomi, P. Frosini, D. Giorgi, and N. Quercioli. Towards a topo-\n",
      "logical–geometrical theory of group equivariant non-expansive operators\n",
      "for data analysis and machine learning. Nature Machine Intelligence, pages\n",
      "1–11, Sept. 2019.\n",
      "[5] T. Besard, C. Foket, and B. De Sutter. Efective extensible programming:\n",
      "Unleashing Julia on GPUs. IEEE Transactions on Parallel and Distributed\n",
      "Systems, 2018.\n",
      "[6] J. Bezanson, A. Edelman, S. Karpinski, and V. B. Shah. Julia: A Fresh Ap-\n",
      "proach to Numerical Computing. SIAM Review, 59(1):65–98, Jan. 2017.\n",
      "[7] J. Chen and J. Revels. Robust benchmarking in noisy environments. arXiv\n",
      "e-prints, Aug 2016.\n",
      "[8] H. Drucker and Y. Le Cun. Double backpropagation increasing general-\n",
      "ization performance. In IJCNN-91-Seattle International Joint Conference on\n",
      "Neural Networks, volume ii, pages 145–150 vol.2, 1991.\n",
      "[9] J. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse,\n",
      "trainable neural networks. arXiv preprint arXiv:1803.03635, 2018.\n",
      "[10] R. Frostig, M. J. Johnson, and C. Leary. Compiling machine learning pro-\n",
      "grams via high-level tracing. Systems for Machine Learning, 2018.\n",
      "[11] S. Gurumurthy, S. Bai, Z. Manchester, and J. Z. Kolter. Joint inference and\n",
      "input optimization in equilibrium networks. Advances in Neural Informa-\n",
      "tion Processing Systems, 34, 2021.\n",
      "25\n",
      "[12] G. Hennequin, T. P. Vogels, and W. Gerstner. Non-normal amplifcation\n",
      "in random balanced neuronal networks. Physical Review E, 86(1):011909,\n",
      "2012.\n",
      "[13] M. Innes, A. Edelman, K. Fischer, C. Rackauckas, E. Saba, V. B. Shah, and\n",
      "W. Tebbutt. A diferentiable programming system to bridge machine learn-\n",
      "ing and scientifc computing. arXiv preprint arXiv:1907.07587, 2019.\n",
      "[14] M. Innes, E. Saba, K. Fischer, D. Gandhi, M. C. Rudilosso, N. M. Joy,\n",
      "T. Karmali, A. Pal, and V. Shah. Fashionable modelling with fux. CoRR,\n",
      "abs/1811.01457, 2018.\n",
      "[15] Y. LeCun, Y. Bengio, et al. Convolutional networks for images, speech,\n",
      "and time series.\n",
      "The handbook of brain theory and neural networks,\n",
      "3361(10):1995, 1995.\n",
      "[16] X. Li, H. Chen, X. Qi, Q. Dou, C.-W. Fu, and P.-A. Heng. H-DenseUNet:\n",
      "Hybrid Densely Connected UNet for Liver and Tumor Segmentation From\n",
      "CT Volumes. IEEE Transactions on Medical Imaging, 37(12):2663–2674, Dec.\n",
      "2018.\n",
      "[17] H. Liu, K. Simonyan, and Y. Yang. Darts: Diferentiable architecture search.\n",
      "In International Conference on Learning Representations, 2018.\n",
      "[18] J. Lorraine, P. Vicol, and D. Duvenaud. Optimizing millions of hyperparam-\n",
      "eters by implicit diferentiation. In International Conference on Artifcial\n",
      "Intelligence and Statistics, pages 1540–1552. PMLR, 2020.\n",
      "[19] W. Moses and V. Churavy. Instead of rewriting foreign code for machine\n",
      "learning, automatically synthesize fast gradients. In H. Larochelle, M. Ran-\n",
      "zato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural In-\n",
      "formation Processing Systems, volume 33, pages 12472–12485. Curran As-\n",
      "sociates, Inc., 2020.\n",
      "[20] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin,\n",
      "A. Desmaison, L. Antiga, and A. Lerer. Automatic diferentiation in py-\n",
      "torch. 2017.\n",
      "[21] A. Paszke, D. Johnson, D. Duvenaud, D. Vytiniotis, A. Radul, M. John-\n",
      "son, J. Ragan-Kelley, and D. Maclaurin. Getting to the point. index sets\n",
      "and parallelism-preserving autodif for pointful array programming. arXiv\n",
      "preprint arXiv:2104.05372, 2021.\n",
      "26\n",
      "[22] C. Rackauckas, Y. Ma, J. Martensen, C. Warner, K. Zubov, R. Supekar,\n",
      "D. Skinner, A. Ramadhan, and A. Edelman. Universal diferential equa-\n",
      "tions for scientifc machine learning. arXiv preprint arXiv:2001.04385, 2020.\n",
      "[23] D. Rumelhart. Learning internal representation by back propagation. Par-\n",
      "allel distributed processing: exploration in the microstructure of cognition, 1,\n",
      "1986.\n",
      "[24] B. Saeta and D. Shabalin. Swift for tensorfow: A portable, fexible platform\n",
      "for deep learning. Proceedings of Machine Learning and Systems, 3, 2021.\n",
      "[25] B. Schölkopf, A. J. Smola, and F. Bach. Learning with Kernels: Support Vector\n",
      "Machines, Regularization, Optimization, and Beyond. MIT Press, 2002.\n",
      "[26] F. Srajer, Z. Kukelova, and A. Fitzgibbon. A benchmark of selected algo-\n",
      "rithmic diferentiation tools on some problems in computer vision and ma-\n",
      "chine learning. Optimization Methods and Software, 33(4-6):889–906, 2018.\n",
      "[27] B. van Merrienboer, O. Breuleux, A. Bergeron, and P. Lamblin. Automatic\n",
      "diferentiation in ml: Where we are and where we should be going. Ad-\n",
      "vances in Neural Information Processing Systems, 31:8757–8767, 2018.\n",
      "[28] D. Varga, A. Csiszárik, and Z. Zombori. Gradient regularization improves\n",
      "accuracy of discriminative models. arXiv preprint arXiv:1712.09936, 2017.\n",
      "[29] P. Vertechi, P. Frosini, and M. G. Bergomi. Parametric machines: a fresh\n",
      "approach to architecture search. arXiv preprint arXiv:2007.02777, 2020.\n",
      "[30] F. Wang, J. Decker, X. Wu, G. Essertel, and T. Rompf. Backpropagation with\n",
      "callbacks: Foundations for efcient and expressive diferentiable program-\n",
      "ming. Advances in Neural Information Processing Systems, 31:10180–10191,\n",
      "2018.\n",
      "[31] P. J. Werbos. Generalization of backpropagation with application to a re-\n",
      "current gas market model. Neural networks, 1(4):339–356, 1988.\n",
      "[32] F. C. White, M. Zgubic, M. Abbott, J. Revels, N. Robinson, A. Arslan, D. Wid-\n",
      "mann, S. Schaub, Y. Ma, willtebbutt, S. Axen, P. Vertechi, C. Rackauckas,\n",
      "K. Fischer, BSnelling, st––, B. Cottier, Jutho, N. Schmitz, B. Chen, C. Vogt,\n",
      "F. Chorney, G. Dhingra, J. Bradbury, J. Sarnof, J. TagBot, M. Protter, M. Be-\n",
      "sançon, M. Schauer, and O. Schulz. Juliadif/chainrulescore.jl: v1.14.0, Mar.\n",
      "2022.\n",
      "27\n",
      "[33] E. Winston and J. Z. Kolter. Monotone operator equilibrium networks.\n",
      "Advances in neural information processing systems, 33:10718–10728, 2020.\n",
      "[34] K. Zubov, Z. McCarthy, Y. Ma, F. Calisto, V. Pagliarino, S. Azeglio, L. Bot-\n",
      "tero, E. Luján, V. Sulzer, A. Bharambe, et al.\n",
      "Neuralpde: Automat-\n",
      "ing physics-informed neural networks (pinns) with error approximations.\n",
      "arXiv preprint arXiv:2107.09443, 2021.\n",
      "A\n",
      "Normed vector spaces and Fréchet derivatives\n",
      "Given normed spaces X1, X2, a function f : X1 → X2 is diferentiable at x1 ∈ X1\n",
      "if it can be locally approximated by a bounded linear operator Df (x1). It is\n",
      "continuously diferentiable if it is diferentiable at all points and the derivative\n",
      "Df : X1 → B(X1, X2) is continuous, where B(X1, X2) is the space of bounded\n",
      "linear operators with operator norm. Whenever that is the case, we will say that\n",
      "f is C1. We will also denote the space of continuously diferentiable functions\n",
      "as C1(X1, X2).\n",
      "We will use ∗ to denote both the dual of a normed space, i.e. X∗ = B(X, R),\n",
      "and the dual of each operator. In particular, Df (x1)∗, the dual of the derivative,\n",
      "will correspond to the operator that backpropagates cotangent vectors from the\n",
      "output space to the input space.\n",
      "The following proposition details alternative conditions which are equiv-\n",
      "alent to requiring that a given continuously diferentiable map f lowers to a\n",
      "continuously diferentiable map ˜f between quotients.\n",
      "Proposition 5. Let X be a normed vector space. Let f ∈ C1(X, X). Let V, W be\n",
      "closed subspaces of X. The following conditions are equivalent.\n",
      "1. f lowers to a map ˜f ∈ C1(X/V, X/W).\n",
      "2. For all x ∈ X, and v ∈ V , f(x + v) − f(x) ∈ W.\n",
      "3. For all x ∈ X, (Df(x))V ⊆ W.\n",
      "4. For all x ∈ X, Df(x) lowers to a map ˜L(x) ∈ B(X/V, X/W).\n",
      "Proof. If item 1 is verifed, that is to say f can be lowered to a quotient map\n",
      "˜f ∈ C1(X/V, X/W), then necessarily, for all v ∈ V , f(x + v) and f(x) cor-\n",
      "respond to the same value module W, hence item 2 is verifed. In item 2, we\n",
      "28\n",
      "can equivalently ask that f(x + λv) − f(x) ∈ W for all λ ∈ R, v ∈ V . Let us\n",
      "consider the quantity\n",
      "f(x + λv) − f(x) =\n",
      "Z λ\n",
      "0\n",
      "d\n",
      "dsf(x + sv)ds =\n",
      "Z λ\n",
      "0\n",
      "Df(x + sv)vds.\n",
      "The integrand Df(x + sv)v is continuous in s, therefore\n",
      "Z λ\n",
      "0\n",
      "Df(x + sv)vd ∈ W for all λ ∈ R, x ∈ X, v ∈ V\n",
      "if and only if\n",
      "Df(x + sv)v ∈ W for all s ∈ R, x ∈ X, v ∈ V\n",
      "or, equivalently,\n",
      "(Df(x))V ⊆ W for all x ∈ X,\n",
      "hence items 2 and 3 are equivalent. By the universal property of the quotient,\n",
      "item 4 is equivalent to item 3, hence items 2 to 4 are equivalent. Whenever they\n",
      "are all true, we can defne the lowered map ˜f ∈ C1(X/V, X/W) as\n",
      "˜f([x]) = [f(x)],\n",
      "which is well defned thanks to item 2 and has a well defned diferential given\n",
      "by D ˜f(x) = ˜L(x) as in item 4. It is straightforward to verify that D ˜f : X/V →\n",
      "B(X/V, X/W) is continuous. Hence, items 2 to 4 imply item 1.\n",
      "B\n",
      "Numerical experiments\n",
      "We ran forward and backward pass of dense, convolutional, and recurrent ma-\n",
      "chines, as described in section 3. The implementation and benchmarking code is\n",
      "implemented in the Julia programming language [6], using Flux.jl [14] for deep\n",
      "learning primitives, CUDA.jl [5] for GPU support, and ChainRulesCore.jl [32]\n",
      "for efcient diferentiation of pointwise activation functions. The code is avail-\n",
      "able at https://github.com/BeaverResearch/ParametricMachinesDemos.jl. Sim-\n",
      "ulations were run on a Intel(R) Core(TM) i7-7700HQ CPU @ 2.80GHz and on\n",
      "a Quadro M1200 GPU. We report the minimum times found benchmarking via\n",
      "the BenchmarkTools package [7], rounded to the ffth signifcant digit, as well\n",
      "as the backward time / forward time ratio, rounded to the third decimal place.\n",
      "The backward pass timings indicate the time to backpropagate cotangent vec-\n",
      "tors from machine space to input space. It is assumed that the forward pass has\n",
      "already been computed and that its result is available.\n",
      "29\n",
      "machine\n",
      "size\n",
      "device\n",
      "forward (ms)\n",
      "backward (ms)\n",
      "ratio\n",
      "dense\n",
      "small\n",
      "CPU\n",
      "22.1\n",
      "17.6\n",
      "0.796\n",
      "dense\n",
      "small\n",
      "GPU\n",
      "806.2\n",
      "936.6\n",
      "1.162\n",
      "dense\n",
      "medium\n",
      "CPU\n",
      "224.3\n",
      "181.9\n",
      "0.811\n",
      "dense\n",
      "medium\n",
      "GPU\n",
      "782.6\n",
      "883.1\n",
      "1.128\n",
      "convolution\n",
      "small\n",
      "CPU\n",
      "100.6\n",
      "93.4\n",
      "0.928\n",
      "convolution\n",
      "small\n",
      "GPU\n",
      "1056.7\n",
      "1131.6\n",
      "1.071\n",
      "convolution\n",
      "medium\n",
      "CPU\n",
      "29504\n",
      "28878\n",
      "0.979\n",
      "convolution\n",
      "medium\n",
      "GPU\n",
      "2054.3\n",
      "2252.9\n",
      "1.097\n",
      "recurrent\n",
      "small\n",
      "CPU\n",
      "365.2\n",
      "334.8\n",
      "0.917\n",
      "recurrent\n",
      "small\n",
      "GPU\n",
      "4427.4\n",
      "4542.2\n",
      "1.026\n",
      "recurrent\n",
      "medium\n",
      "CPU\n",
      "41184\n",
      "40932\n",
      "0.994\n",
      "recurrent\n",
      "medium\n",
      "GPU\n",
      "7058.7\n",
      "7118.5\n",
      "1.008\n",
      "Table 1: Timings of forward and backward passes of dense, convolu-\n",
      "tional, and recurrent machines, and backward over forward ratio. We\n",
      "benchmarked on a single minibatch for a small problem size (each index set Ii\n",
      "has dimension 2, the minibatch contains 2 samples) and a medium problem size\n",
      "(each index set Ii has dimension 32, the minibatch contains 32 samples).\n",
      "30\n",
      "\n",
      "The document is about a mathematical framework for neural networks.\n",
      "\n",
      "Decision: Excluded.\n",
      "\n",
      "Reason: The study design is not a randomized controlled trial (RCT), and the population does not involve patients with post-traumatic stress disorder or any medical condition; it is a theoretical and mathematical analysis.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from openai import AzureOpenAI\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import fitz\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            # Convert page to image\n",
    "            pix = page.get_pixmap()\n",
    "            img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "            \n",
    "            # Perform OCR on the image\n",
    "            ocr_text = pytesseract.image_to_string(img)\n",
    "            \n",
    "            # Append OCR text to the result\n",
    "            text += ocr_text\n",
    "    return text\n",
    "\n",
    "# Path to your PDF file\n",
    "pdf_path = 'md.pdf'\n",
    "\n",
    "# Extracted text from PDF\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Print or process the extracted text\n",
    "print(pdf_text)\n",
    "\n",
    "\n",
    "\n",
    "# Your message text\n",
    "message_text ={\n",
    "    \"role\":\"system\",\n",
    "\"content\": '''\n",
    "    You are an expert systematic literature reviewer. This is the Full Text Screening phase. You have to analyze the entire document provided to you. Based on the below exclusion and inclusion criteria, please classify the given citation as 'Included' or 'Excluded'. Apply the Exclusion Criteria first. If any statement in the Exclusion Criteria is true, mark the citation as 'Excluded'. If the citation passes the Exclusion Criteria, then check the Inclusion Criteria. Mark the citation as 'Included' only if it strictly and exactly passes all Inclusion Criteria statements; otherwise, mark the citation as 'Excluded'. \n",
    "\n",
    "    Inclusion Criteria:\n",
    "    1. Study Design: Only randomized controlled trials (RCTs) are included. No other study design is acceptable.\n",
    "    2. Condition: The study must explicitly mention 'post-traumatic stress disorder'.\n",
    "    3. Medications: Include studies that primarily investigate the utilization of the following medications: sertraline, 'sertraline plus brexpiprazole', fluoxetine, paroxetine, and venlafaxine. These medications are permissible for use either as standalone treatments (monotherapy) or in combination therapy. However, in cases of combination therapy, it is imperative that the combination exclusively involves the aforementioned medications.\n",
    "    4. Population: The study must involve adult patients aged 18 years and older.\n",
    "    5. If the trial is assessing more than two interventions in different arms of the randomized control trial, at least two interventions should be from sertraline, 'sertraline plus brexpiprazole', fluoxetine, paroxetine, and venlafaxine\n",
    "\n",
    "    Exclusion Criteria:\n",
    "    1. Study Type: Literature reviews, meta-analyses, systematic reviews, review, narrative review, observational studies (including retrospective, case-control, cohort studies, pharmacokinetic, pharmacodynamics study, cost-effectiveness, economic evaluation studies, longitudinal studies, and cross-sectional studies).\n",
    "    2. Population: Studies exclusively involving animals, adolescents, children, or populations other than those with post-traumatic stress disorder.\n",
    "    3. Medications: Exclude studies that do not primarily investigate the utilization of the following medications: sertraline, 'sertraline plus brexpiprazole', fluoxetine, paroxetine, and venlafaxine. These medications are permissible for use either as standalone treatments (monotherapy) or in combination therapy. However, in cases of combination therapy, it is imperative that the combination exclusively involves the aforementioned medications.\n",
    "    4. Strictly mark the citation as excluded if it mentions review or pooled analysis which includes data from more than one study.\n",
    "    5. If the study design is not clearly randomized controlled trials (RCTs) then exclude the study.\n",
    "    \n",
    "    in response return what the document is all about in 10-15 words and your decision with breif reason (study design, population) why you took this decision\n",
    "    '''\n",
    "}\n",
    "# Combine message text with PDF text\n",
    "combined_text = [{\"role\": \"system\", \"content\": pdf_text},message_text]\n",
    "\n",
    "# Send combined text to OpenAI API\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint = \"https://pcefrance.openai.azure.com/\",\n",
    "    api_key=\"668a69822708413e90117aea513b8dda\",\n",
    "    api_version=\"2024-02-15-preview\")\n",
    "\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"GPT180\",\n",
    "    messages = combined_text,  \n",
    "    temperature=0.7,\n",
    "    max_tokens=800,\n",
    "    top_p=0.95,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None)\n",
    "\n",
    "print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SLR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
